{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597fdd84",
   "metadata": {},
   "source": [
    "# 문자 RNN으로 성씨 국적 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ea324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa774b0d",
   "metadata": {},
   "source": [
    "## `Vocabulary`, `Vectorizer`, `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f043787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" 매핑을 위해 텍스트 처리하고 어휘 사전 만드는 클래스 \"\"\"\n",
    "    \n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        token_to_idx: 기존 토큰-인덱스 매핑 딕셔너리\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        \n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx:token for token, idx in \n",
    "                              self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" 직렬화 가능한 딕셔너리 반환 \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체 만듦 \"\"\"\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    # 클래스 자체의 내용 출력 메소드\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3846b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성씨 문자열의 정수 매핑을 위한 SequenceVocabulary 클래스 \n",
    "# unk_token, mask_token 등을 사용해서 처리함.\n",
    "\n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                end_seq_token=\"<END>\"):\n",
    "        \n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "        \n",
    "        self.mask_index = self.add_token(self._mask_token) # 0\n",
    "        self.unk_index = self.add_token(self._unk_token) # 1\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token) # 2\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token) # 3\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        # 상속한 Vocabulary객체의 to_serializable() 결과 {'token_to_idx':{__}} 이어받음\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token':self._unk_token,\n",
    "                        'mask_token':self._mask_token,\n",
    "                        'begin_seq_token':self._begin_seq_token,\n",
    "                        'end_seq_token':self._end_seq_token})\n",
    "        return contents\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            # 해당 토큰이 없으면 unk_index 1 반환\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b34a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    \"\"\" 어휘 사전 생성 및 관리 \"\"\"\n",
    "    \n",
    "    def __init__(self, char_vocab, nationality_vocab):\n",
    "        self.char_vocab = char_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        \n",
    "    def vectorize(self, surname, vector_length=-1):\n",
    "        indices = [self.char_vocab.begin_seq_index] # [2]\n",
    "        indices.extend(self.char_vocab.lookup_token(token)\n",
    "                      for token in surname) # [2, ...]\n",
    "        indices.append(self.char_vocab.end_seq_index) # [2, ..., 3]\n",
    "        \n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices) # 벡터 길이: 성씨 문자열 길이 + 2\n",
    "            \n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64) \n",
    "        # ex. array([0, 0, 0, 0, 0], dtype=int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.char_vocab.mask_index # 남은 칸은 0 마스킹\n",
    "        \n",
    "        return out_vector, len(indices)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        char_vocab = SequenceVocabulary()\n",
    "        nationality_vocab = Vocabulary()\n",
    "        \n",
    "        for index, row in surname_df.iterrows():\n",
    "            for char in row.surname:\n",
    "                char_vocab.add_token(char)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "            \n",
    "        return cls(char_vocab, nationality_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\n",
    "        nat_vocab = Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        \n",
    "        return cls(char_vocab=char_vocab, nationality_vocab=nat_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'char_vocab': self.char_vocab.to_serializable(),\n",
    "               'nationality_vocab': self.nationality_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a3075fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2  # 19\n",
    "        \n",
    "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size = len(self.train_df)  # 7680\n",
    "        \n",
    "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)  # 1640\n",
    "        \n",
    "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size = len(self.test_df)  # 1660\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                            'val': (self.val_df, self.validation_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "        self.set_split('train')\n",
    "        \n",
    "        # 클래스 가중치\n",
    "        class_counts = self.train_df.nationality.value_counts().to_dict()\n",
    "        \n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0]) # 알파벳 순 \n",
    "        \n",
    "        # 국가 알파벳 순으로 국가 빈도 정렬\n",
    "        sorted_counts = sorted(class_counts.items(), key = sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "#         tensor([0.0009, 0.0065, 0.0035, 0.0061, 0.0005, 0.0063, 0.0025, 0.0092, 0.0078,\n",
    "#         0.0024, 0.0018, 0.0189, 0.0119, 0.0263, 0.0006, 0.0192, 0.0056, 0.0250])\n",
    "        \n",
    "        # 클래스 가중치 총합: tensor(0.1548)\n",
    "        \"\"\" \n",
    "        클래스 범주 빈도 낮을수록, 높은 가중치\n",
    "        빈도 높을수록, 낮은 가중치 ----> 왜 사용할까?\n",
    "        \"\"\"\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, 'w') as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        surname_vector, vec_length = \\\n",
    "           self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
    "        \n",
    "        nationality_index = \\\n",
    "           self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "        \n",
    "        return {'x_data':surname_vector,\n",
    "               'y_target':nationality_index,\n",
    "               'x_length':vec_length}\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, \n",
    "                     drop_last=True, device='cpu'):\n",
    "    \"\"\"\n",
    "    DataLoader로 미니배치로 모은 후,\n",
    "    CPU와 GPU 간 데이터를 간편하게 전환하는 제너레이터 \n",
    "    - 각 텐서를 지정된 장치로 이동시킴.\n",
    "    - 반환 타입: generator <_____>\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                           shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c5d55",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a1d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_gather(y_out, x_lengths):\n",
    "    \"\"\"\n",
    "    y_out에 있는 각 데이터포인트에서 마지막 벡터 추출\n",
    "    \n",
    "    배치 행 인덱스를 순회하며 x_lengths \n",
    "    값에 해당하는 인덱스 위치의 벡터 반환\n",
    "    \n",
    "    y_out: (batch, sequence, feature)\n",
    "    x_lengths: (batch,)\n",
    "    \n",
    "    반환값 y_out: (batch, feature)\n",
    "    \"\"\"\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "    \n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "        \n",
    "    return torch.stack(out)\n",
    "\n",
    "class ElmanRNN(nn.Module):\n",
    "    \"\"\" RNNCell 사용하여 만든 엘만 RNN \n",
    "    --> RNN 계산 명시적으로 드러내기 위함 \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        \"\"\"\n",
    "        input_size: 입력 벡터 크기\n",
    "        hidden_size: 은닉 상태 벡터 크기 \n",
    "        batch_first: 0번째 차원이 배치인지 여부 \n",
    "        \"\"\"\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        \n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        # 입력-은닉 가중치 행렬, 은닉-은닉 가중치 행렬 만듦\n",
    "        # 입력 벡터 행렬과 은닉 벡터 행렬을 받음\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def _initial_hidden(self, batch_size):\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "    \n",
    "    def forward(self, x_in, initial_hidden = None):\n",
    "        \"\"\"\n",
    "        ElmanRNN의 정방향 계산\n",
    "        \n",
    "        x_in: 입력 데이터 텐서\n",
    "        - 만약 batch_first==True면, (batch_size, seq_size, feature_size)\n",
    "        - 아니면, (seq_size, batch_size, feature_size)\n",
    "        initial_hidden: RNN의 초기 은닉 상태\n",
    "        \n",
    "        반환값 hiddens: 각 타임 스텝에서 RNN 출력 \n",
    "        - 만약 batch_first==True면, (batch_size, seq_size, hidden_size)\n",
    "        - 아니면, (seq_size, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1,0,2)  # (seq_size, batch_size, feature_size) 형태로 돌려놓음\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "        \n",
    "        hiddens = []  # 각 타임스텝의 은닉벡터 결과행렬을 담을 리스트\n",
    "        \n",
    "        if initial_hidden is None: # 아직 진행이 되지 않았다면 \n",
    "            initial_hidden = self._initial_hidden(batch_size)\n",
    "            # (배치크기 * 은닉크기) 모양의 초기 은닉행렬 만듦\n",
    "            initial_hidden = initial_hidden.to(x_in.device)\n",
    "            \"\"\" 초기 은닉 상태 벡터는 모두 0 \"\"\"\n",
    "        hidden_t = initial_hidden\n",
    "        \n",
    "        \"\"\" 입력 벡터의 길이만큼 RNNCell 반복 \"\"\"\n",
    "        for t in range(seq_size):\n",
    "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
    "            \"\"\" 현재 타임 스텝의 은닉 벡터는 이전 타임 스텝의 은닉 벡터와 \n",
    "            현재 입력 벡터를 가지고 만들어짐 \"\"\"\n",
    "            hiddens.append(hidden_t)\n",
    "            \"\"\" 각 타임 스텝의 은닉 벡터 결과를 hiddens 리스트에 추가 \"\"\"\n",
    "        \n",
    "        hiddens = torch.stack(hiddens)\n",
    "        # 은닉 벡터 행렬을 모두 쌓아서 3차원 텐서를 만듦\n",
    "        \n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1,0,2)\n",
    "            # 0차원(seq_size)과 1차원(batch_size) 바꾸기\n",
    "            # 또다시 (batch_size, seq_size, feat_size) 로 돌려놓음\n",
    "        \n",
    "        return hiddens\n",
    "    \n",
    "class SurnameClassifier(nn.Module):\n",
    "    \"\"\" RNN으로 특성 추출하고 MLP로 분류하는 분류 모델 \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
    "                rnn_hidden_size, batch_first=True, padding_idx=0):\n",
    "        \"\"\"\n",
    "        embedding_size: 문자 임베딩 크기\n",
    "        num_embeddings: 임베딩할 문자 개수\n",
    "        num_classes: 예측 벡터 크기 (국적 개수)\n",
    "        rnn_hidden_size: RNN의 은닉 상태 크기\n",
    "        batch_first: 입력 텐서의 0번째 차원이 배치인지 시퀀스인지\n",
    "        padding_idx: 텐서 패딩을 위한 인덱스\n",
    "        \"\"\"\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                               embedding_dim=embedding_size,\n",
    "                               padding_idx=padding_idx)\n",
    "        # classifier.emb: Embedding(80, 100, padding_idx=0)\n",
    "        # - 타입: torch.nn.modules.sparse.Embedding\n",
    "        \n",
    "        self.rnn = ElmanRNN(input_size=embedding_size,\n",
    "                           hidden_size=rnn_hidden_size,\n",
    "                           batch_first=batch_first)\n",
    "        # classifier.rnn : ElmanRNN( (rnn_cell): RNNCell(100, 64) )\n",
    "        # - 타입: __main__.ElmanRNN\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
    "                            out_features=rnn_hidden_size)\n",
    "        # classifier.fc1 : Linear(in_features=64, out_features=64, bias=True)\n",
    "        # - 타입 : torch.nn.modules.linear.Linear\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
    "                             out_features=num_classes)\n",
    "        # classifier.fc2 : Linear(in_features=64, out_features=18, bias=True)\n",
    "        # - 타입 : torch.nn.modules.linear.Linear\n",
    "        \n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        \"\"\" 분류기의 정방향 계산\n",
    "        \n",
    "        x_in: (batch, input_dim) 크기\n",
    "        x_lengths: 배치에 있는 각 시퀀스의 길이\n",
    "        - 시퀀스의 마지막 벡터를 찾는데 사용\n",
    "        \n",
    "        반환 텐서: (batch, output_dim) 크기\n",
    "        \"\"\"\n",
    "        x_embedded = self.emb(x_in)\n",
    "        y_out = self.rnn(x_embedded)\n",
    "        \n",
    "        if x_lengths is not None:\n",
    "            y_out = column_gather(y_out, x_lengths)\n",
    "        else:\n",
    "            y_out = y_out[:, -1, :] \n",
    "            # 각 시퀀스의 마지막 벡터 행렬\n",
    "            # 최종 은닉 벡터 행렬\n",
    "            \n",
    "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
    "        # 선형층1 --> 비선형 활성화 함수 relu\n",
    "        \n",
    "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
    "        # --> 선형층2\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf39cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1704510",
   "metadata": {},
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5c451c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용여부: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\\\n",
    "    \n",
    "    # 날짜와 경로 정보\n",
    "    surname_csv = \"data/surnames/surnames_with_splits.csv\",\n",
    "    vectorizer_file = \"vectorizer.json\",\n",
    "    model_state_file = \"model.pth\",\n",
    "    save_dir = \"model_storage/ch6/surname_classification\",\n",
    "    \n",
    "    # 모델 하이퍼파라미터\n",
    "    char_embedding_size = 100,\n",
    "    rnn_hidden_size = 64,\n",
    "    \n",
    "    # 훈련 하이퍼파라미터\n",
    "    num_epochs = 100,\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 64,\n",
    "    seed = 1337,\n",
    "    early_stopping_criteria = 5,\n",
    "    \n",
    "    # 실행 옵션\n",
    "    cuda = True,\n",
    "    catch_keyboard_interrupt = True,\n",
    "    reload_from_files = False,\n",
    "    expand_filepaths_to_save_dir = True)\n",
    "\n",
    "\n",
    "# CUDA 체크\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else 'cpu')\n",
    "\n",
    "print(\"CUDA 사용여부: {}\".format(args.cuda))\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                       args.vectorizer_file)\n",
    "    \n",
    "    args.model_state_file = os.path.join(args.save_dir, \n",
    "                                         args.model_state_file)\n",
    "\n",
    "    \n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 디렉토리 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9157db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "    # 체크포인트 로드 \n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
    "                                                             args.vectorizer_file)\n",
    "else:\n",
    "    # 데이터셋과 vectorizer 로드\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    \n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = SurnameClassifier(embedding_size = args.char_embedding_size, # 100 임베딩 차원(크기)\n",
    "                              num_embeddings = len(vectorizer.char_vocab), # 80 임베딩 개수 (문자 개수)\n",
    "                              num_classes = len(vectorizer.nationality_vocab), # 18 국가 클래스(범주)\n",
    "                              rnn_hidden_size = args.rnn_hidden_size,  # 64 은닉 벡터 행렬 크기\n",
    "                              padding_idx = vectorizer.char_vocab.mask_index) # 0 마스킹(패딩) 넘버"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6aa5c3",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fc76bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop early':False,\n",
    "           'early_stopping_step':0,\n",
    "           'early_stopping_best_val':1e8, # 100000000.0\n",
    "           'learning_rate': args.learning_rate, # 1e-3 (0.001)\n",
    "           'epoch_index':0,\n",
    "           'train_loss':[], # 각 epoch의 훈련 손실 \n",
    "           'train_acc':[], # 각 epoch의 훈련 정확도\n",
    "           'val_loss':[],  # 각 epoch의 val 손실 \n",
    "           'val_acc':[],   # 각 epoch의 val 정확도\n",
    "           'test_loss':-1,\n",
    "           'test_acc':-1,\n",
    "           'model_filename': args.model_state_file} # \"model.pth\"\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\" 훈련 상태 업데이트\n",
    "    \n",
    "    콤포넌트:\n",
    "    - 조기 종료: 과대 적합 방지\n",
    "    - 모델 체크포인트: 더 나은 모델 저장\n",
    "    \n",
    "    param args: 메인 매개변수\n",
    "    param model: 훈련할 모델\n",
    "    param train_state: 훈련 상태를 담은 딕셔너리\n",
    "    \n",
    "    returns: 새로운 훈련 상태\n",
    "    \"\"\"\n",
    "    \n",
    "    # 적어도 한번 모델 저장\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "        \n",
    "    # 성능이 향상되면 모델 저장\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "        \n",
    "        # 손실이 나빠지면 \n",
    "        if loss_t >= loss_tm1:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "            \"\"\" \n",
    "            이전 val_loss 보다 현재 val_loss가 더 크면 \n",
    "            early_stopping_step을 1씩 증가시켜 주는데,\n",
    "            이게 5가 넘어가면 stop_early 항목을 True로 바꿔줌\n",
    "            --> 조기 종료\n",
    "            \"\"\"\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장 \n",
    "            # 첨에 100000000.0 보다 작으면 해당 손실로 업데이트\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "            \n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "            \"\"\"\n",
    "            손실이 한번이라도 감소하게 되면 \n",
    "            조기 종료 단계를 다시 0으로 초기화 시킴 \n",
    "            --> 이후에 다시 5번 손실 증가돼야 조기 종료\n",
    "            \"\"\"\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "           train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "        # 5 이상이면 조기 종료시킴\n",
    "        \n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfbe0202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ef685b01564e7abd93e5c38de46419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03c2c598b334c3c93b3672245cf6236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6016902cddff4d58ae257d1e18a0cbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights) # 가중손실함수 \n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                mode='min', factor=0.5,\n",
    "                                                patience=1)\n",
    "\n",
    "train_state = make_train_state(args) # 훈련 상태 딕셔너리 만듦\n",
    "\n",
    "epoch_bar = tqdm.notebook.tqdm(desc='training routine',\n",
    "                              total=args.num_epochs,\n",
    "                              position=0) # 바깥 루프에 대한 position: 0\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm.notebook.tqdm(desc='split=train',\n",
    "                              total=dataset.get_num_batches(args.batch_size),\n",
    "                              position=1, # 안쪽 루프에 대한 position: 1\n",
    "                              leave=True) # 루프 완료시 진행률 그대로 남김\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm.notebook.tqdm(desc='split=val',\n",
    "                            total=dataset.get_num_batches(args.batch_size),\n",
    "                            position=1,  # 안쪽 루프에 대한 position: 1\n",
    "                            leave=True) # 루프 완료시 진행률 그대로 남김\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index # epoch 회차 설정\n",
    "        \n",
    "        # 훈련 세트에 대한 순회\n",
    "        \n",
    "        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                          batch_size=args.batch_size,\n",
    "                                          device=args.device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()  # 모델을 훈련 모드로 설정\n",
    "        \n",
    "        # 총 120회 훈련 데이터셋 미니배치 루프 돎 \n",
    "        # 각 배치를 돌때마다 손실 계산, 가중치 업데이트 (1epoch당 120번)\n",
    "        for batch_index, batch_dict in enumerate(batch_generator): # 0~119\n",
    "            # 1. 그래디언트 0 초기화\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 2. 출력 계산\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'],\n",
    "                               x_lengths=batch_dict['x_length'])\n",
    "            \n",
    "            # 3. 손실 계산\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            \n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            # 4. 손실을 사용해 그래디언트 계산\n",
    "            loss.backward()\n",
    "            \n",
    "            # 5. 옵티마이저로 가중치 업데이트\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 정확도 계산\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # 진행 상태 막대 업데이트\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                  epoch = epoch_index)\n",
    "            train_bar.update()\n",
    "            \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "        \n",
    "        # 검증 세트에 대한 순회\n",
    "        \n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                          batch_size=args.batch_size,\n",
    "                                          device=args.device)\n",
    "        \n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "        \"\"\" 모델을 평가 모드로 설정하여 역전파 끔 \n",
    "        --> 편향되지 않은 모델의 성능을 얻는 목적으로만 사용 \"\"\"\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 1. 출력 계산\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'],\n",
    "                               x_lengths=batch_dict['x_length'])\n",
    "            \n",
    "            # 2. 손실 계산\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            # 3. 정확도 계산\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            # 평균 정확도? ex. 0.90 --> 0.05 / 2 (0.025 --> 0.925) \n",
    "            # 0.035 / 3 (0.011 --> 0.936)\n",
    "            # 정확도 증감분 / 배치 진도 --> 를 더해줌 (점진적 증감)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "            \n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        \n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                        train_state=train_state)\n",
    "        \n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "        \n",
    "        # 각 epoch 끝날때 train_bar과 val_bar 상태를 0으로 초기화시켜줌\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        \n",
    "        # epoch_bar 업데이트\n",
    "        epoch_bar.update()\n",
    "        \n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"반복 중지\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69890856",
   "metadata": {},
   "source": [
    "train set 보다 validation set 에서 손실이 더 높고, 정확도가 더 낮음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125cab6",
   "metadata": {},
   "source": [
    "## 모델 검증\n",
    "### 테스트 세트 손실, 정확도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06c5af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 1.8592317199707034;\n",
      "테스트 정확도: 45.00000000000001\n"
     ]
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "# 모델.load_state_dict(torch.load(저장한 모델 파일 경로))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred =  classifier(batch_dict['x_data'],\n",
    "                         x_lengths=batch_dict['x_length'])\n",
    "    \n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "\n",
    "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9394216",
   "metadata": {},
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "83cbbffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nationality': 'Irish', 'probability': 0.5733259320259094, 'surname': 'McMahan'}\n",
      "{'nationality': 'Japanese', 'probability': 0.8905038833618164, 'surname': 'Nakamoto'}\n",
      "{'nationality': 'Chinese', 'probability': 0.3168955445289612, 'surname': 'Wan'}\n",
      "{'nationality': 'Vietnamese', 'probability': 0.3496793806552887, 'surname': 'Cho'}\n"
     ]
    }
   ],
   "source": [
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "    vectorized_surname, vec_length = vectorizer.vectorize(surname)\n",
    "    # ex. vectorizer.vectorize('Wan')\n",
    "    # --> (array([ 2, 26,  7, 25,  3], dtype=int64), 5)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=0)\n",
    "    # ex. tensor([[ 2, 26,  7, 25,  3]])\n",
    "    vec_length = torch.tensor([vec_length], dtype=torch.int64) # tensor([5])\n",
    "    \n",
    "    result = classifier(vectorized_surname, vec_length, apply_softmax=True)\n",
    "#     tensor([[0.0389, 0.3525, 0.0098, 0.0061, 0.0325, 0.0023, 0.0091, 0.0007, 0.0112,\n",
    "#          0.0013, 0.0432, 0.1932, 0.0135, 0.0018, 0.0235, 0.0087, 0.0025, 0.2493]],\n",
    "#        grad_fn=<SoftmaxBackward>)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "#     torch.return_types.max(\n",
    "#         values=tensor([0.4360], grad_fn=<MaxBackward0>),\n",
    "#         indices=tensor([17]))\n",
    "   \n",
    "    \"\"\" 분류 결과가 매번 달라짐 주의!!!! \"\"\"\n",
    "    \n",
    "    index = indices.item() # 17\n",
    "    prob_value = probability_values.item() # 0.4360\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': prob_value, 'surname': surname}\n",
    "\n",
    "classifier = classifier.to(\"cpu\")\n",
    "for surname in ['McMahan', 'Nakamoto', 'Wan', 'Cho']:\n",
    "    print(predict_nationality(surname, classifier, vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf95293",
   "metadata": {},
   "source": [
    "성씨 'Wan'이랑 'Cho'는 'Korean', 'Chinese', 'Vietnamese' 사이에서 헷갈리는 모양!!\n",
    "- 국적 예측 결과가 매번 달라짐 !!\n",
    "- 테스트셋의 정확도가 45% 밖에 되지 않은 것 감안!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
