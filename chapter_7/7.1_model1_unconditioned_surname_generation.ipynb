{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67080606",
   "metadata": {},
   "source": [
    "# 문자 GRU로 성씨 생성 (NLG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c65eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0333a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx:token for token, idx in \n",
    "                              self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            \n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b7b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                end_seq_token=\"<END>\"):\n",
    "        \n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "        \n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                        'mask_token': self._mask_token,\n",
    "                        'begin_seq_token': self._begin_seq_token,\n",
    "                        'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b876458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    \n",
    "    def __init__(self, char_vocab, nationality_vocab):\n",
    "        self.char_vocab = char_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        \n",
    "    def vectorize(self, surname, vector_length=-1):\n",
    "        \"\"\"\n",
    "        성씨를 샘플과 타깃 벡터로 변환\n",
    "        성씨 벡터를 두개의 벡터 surname[:-1], surname[1:]로 나눠 출력\n",
    "        각 타임스텝에서 첫번째 벡터가 샘플, 두번째 벡터가 타깃임\n",
    "        (*여기선 국가범주가 타깃이 아님!!)\n",
    "        \n",
    "        *앞선 엘만RNN 다중분류 문제에서는,\n",
    "        vectorize 함수에서 (성씨벡터, 벡터길이) 이렇게만 해줬음\n",
    "        \n",
    "        - from_vector: 샘플 벡터 (넘파이 배열)\n",
    "        - to_vector: 타깃 벡터 (넘파이 배열)\n",
    "        \"\"\"\n",
    "        indices = [self.char_vocab.begin_seq_index]\n",
    "        indices.extend(self.char_vocab.lookup_token(token) for token in surname)\n",
    "        indices.append(self.char_vocab.end_seq_index)\n",
    "        \n",
    "        # 성씨 문자열 벡터 최대값이 지정되지 않았다면\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices) - 1\n",
    "            \n",
    "        from_vector = np.empty(vector_length, dtype=np.int64)\n",
    "        # ex. array([4294967296, 140733313414141, ...], dtype=int64)\n",
    "        # 배열 값들을 0으로 초기화하지 않아서 더 빠름.\n",
    "        \n",
    "        from_indices = indices[:-1]\n",
    "        from_vector[:len(from_indices)] = from_indices\n",
    "        from_vector[len(from_indices):] = self.char_vocab.mask_index\n",
    "        \n",
    "        \"\"\" 바로 그 다음 문자를 \"예측할 시퀀스\"(타깃 벡터)로 지정 \"\"\"\n",
    "        to_vector = np.empty(vector_length, dtype=np.int64)\n",
    "        to_indices = indices[1:]\n",
    "        to_vector[:len(to_indices)] = to_indices\n",
    "        to_vector[len(to_indices):] = self.char_vocab.mask_index\n",
    "        \n",
    "        return from_vector, to_vector\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        char_vocab = SequenceVocabulary()\n",
    "        nationality_vocab = Vocabulary()\n",
    "        \n",
    "        for index, row in surname_df.iterrows():\n",
    "            for char in row.surname:\n",
    "                char_vocab.add_token(char)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "            \n",
    "        return cls(char_vocab, nationality_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\n",
    "        nat_vocab = Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        \n",
    "        return cls(char_vocab, nat_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'char_vocab': self.char_vocab.to_serializable(),\n",
    "               'nationality_vocab': self.nationality_vocab.to_serializable()}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff6f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2\n",
    "        \n",
    "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                            'val': (self.val_df, self.validation_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "        self.set_split('train')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(surname_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, 'w') as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        from_vector, to_vector = \\\n",
    "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
    "        \n",
    "        nationality_index = \\\n",
    "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "        \n",
    "        return {'x_data': from_vector, # 샘플 벡터 ex. (M,o,o)\n",
    "               'y_target': to_vector,  # 타깃 벡터 ex. (o,o,n)\n",
    "               'class_index': nationality_index}\n",
    "    \"\"\"\n",
    "    앞선 \"시퀀스 분류\" 예제의 데이터셋 반환값은,\n",
    "    \n",
    "    return {'x_data': surname_vector, \n",
    "                'y_target': nationality_index, \n",
    "                'x_length': vec_length}\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "            \n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a40b2b",
   "metadata": {},
   "source": [
    "## 모델: SurnameGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f2472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "모델1: 국적 정보를 사용하지 않는 \"조건 없는\" 모델\n",
    "- GRU가 어떤 국적에도 편향된 계산을 수행하지 않는다! \n",
    "- 초기 은닉 상태 벡터가 계산에 영향 안주도록 모두 0 초기화\n",
    "\"\"\"\n",
    "\n",
    "class SurnameGenerationModel(nn.Module):\n",
    "    def __init__(self, char_embedding_size, char_vocab_size, rnn_hidden_size,\n",
    "                batch_first=True, padding_idx=0, dropout_p=0.5):\n",
    "        \"\"\"\n",
    "        char_embedding_size (int): 문자 임베딩 크기\n",
    "        char_vocab_size (int): 임베딩될 문자 개수 (= num_embeddings)\n",
    "        rnn_hidden_size (int): RNN의 은닉 상태 크기\n",
    "        batch_first (bool): 0번째 차원이 배치인지 시퀀스인지 나타내는 플래그\n",
    "        padding_idx (int): 텐서 패딩을 위한 인덱스\n",
    "        dropout_p (float): 드롭아웃으로 활성화 출력을 0으로 만들 확률\n",
    "        \"\"\"\n",
    "        super(SurnameGenerationModel, self).__init__()\n",
    "        \n",
    "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size, # 임베딩할 문자 개수\n",
    "                                    embedding_dim=char_embedding_size, # 임베딩 크기\n",
    "                                    padding_idx=padding_idx)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=char_embedding_size,\n",
    "                         hidden_size=rnn_hidden_size,\n",
    "                         batch_first=batch_first)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
    "                           out_features=char_vocab_size)\n",
    "        \n",
    "        self._dropout_p = dropout_p\n",
    "        \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        # x_in : 2차원 행렬\n",
    "        # - 초기) tensor([[2]])\n",
    "        \n",
    "        x_embedded = self.char_emb(x_in)\n",
    "        # torch.Size([1, 1, 32]) - 3차원 텐서\n",
    "        \n",
    "        y_out, _ = self.rnn(x_embedded)\n",
    "        \n",
    "        batch_size, seq_size, feat_size = y_out.shape # ex. [1, 1, 32]\n",
    "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "        \n",
    "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
    "        \"\"\" 선형 층의 입력으로는 2차원 행렬임!! \"\"\"\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        new_feat_size = y_out.shape[-1]  # 88\n",
    "        y_out = y_out.view(batch_size, seq_size, new_feat_size) # 다시 3차원 텐서로\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "178241bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, vectorizer, num_samples=1, sample_size=20,\n",
    "                     temperature=1.0):\n",
    "    \"\"\"\n",
    "    모델이 만든 인덱스 시퀀스를 샘플링\n",
    "    \n",
    "    sample_size: 샘플의 최대 길이\n",
    "    temperature: 무작위성 정도 (float)\n",
    "    - 0 < temp < 1.0 : 최대값 선택할 가능성 높음\n",
    "    - temp > 1.0 : 균등 분포에 가까움\n",
    "    \n",
    "    반환값\n",
    "    - indices : 인덱스 행렬\n",
    "    - shape - (num_samples, sample_size)\n",
    "    \"\"\"\n",
    "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index \n",
    "                       for _ in range(num_samples)] # 샘플 개수 2개\n",
    "    begin_seq_index = torch.tensor(begin_seq_index,\n",
    "                                  dtype=torch.int64).unsqueeze(dim=1)\n",
    "    indices = [begin_seq_index]\n",
    "#     [tensor([[2],\n",
    "#              [2]])]\n",
    "    \n",
    "    h_t = None\n",
    "    \"\"\" 초기 은닉 상태 벡터를 모두 0 으로 초기화\n",
    "    --> 국적에 편향되지 않도록, 계산에 영향을 미치지 않음 \"\"\"\n",
    "    \n",
    "    for time_step in range(sample_size): # 0~19\n",
    "        x_t = indices[time_step]\n",
    "        \n",
    "        x_emb_t = model.char_emb(x_t)\n",
    "        # torch.Size([2, 1, 32])\n",
    "        \n",
    "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
    "        # 첫 타임스텝에는 model.rnn(x_emb_t, None)\n",
    "        # rnn_out_t, h_t 은 둘다 [2, 1, 32] 크기, 값도 똑같음\n",
    "        \n",
    "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1)) # [2, 32]\n",
    "        # torch.Size([2, 88])\n",
    "        \n",
    "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
    "        \n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "#        tensor([[29],\n",
    "#                [17]])  리스트에 추가 --> 다음 임베딩의 입력(x_t)으로 들어감\n",
    "    \n",
    "    indices = torch.stack(indices).squeeze().permute(1,0)\n",
    "#     tensor([[[ 2],  --> squeeze() --> permute(1,0)\n",
    "#              [ 2]],   \n",
    "#                        tensor([[ 2,  2],    tensor([[ 2, 29, ...],\n",
    "#                                                     [ 2, 17, ...]])\n",
    "#                                [29, 17],\n",
    "#                                    ...]])\n",
    "#            [[29],\n",
    "#             [17]]])\n",
    "\n",
    "#     [2, 2, 1] 크기 --> [2, 2]...[20, 2] --> [2, 20] --> 2차원 \"행렬\"\n",
    "    \n",
    "    return indices  # 행렬 : (샘플개수_2, 샘플크기_20)\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    \"\"\"\n",
    "    인덱스를 성씨 문자열로 반환\n",
    "    \n",
    "    sampled_indices: 'sample_from_model' 함수에서 얻은 인덱스 텐서\n",
    "    \"\"\"\n",
    "    decoded_surnames = []\n",
    "    vocab = vectorizer.char_vocab\n",
    "    \n",
    "    for sample_index in range(sampled_indices.shape[0]): # 0~1\n",
    "        surname = ''\n",
    "        \n",
    "        for time_step in range(sampled_indices.shape[1]): # 0~19\n",
    "            \n",
    "            sample_item = sampled_indices[sample_index, time_step].item()\n",
    "            # sampled_indices[0,0]--> [0,1]--> [0,2]--> ...--> [0,19]\n",
    "            # 중간에 0(end_seq_index) 나오면 바로 다음 샘플로\n",
    "            \n",
    "            if sample_item == vocab.begin_seq_index:\n",
    "                continue\n",
    "            elif sample_item == vocab.end_seq_index:\n",
    "                break\n",
    "            else:\n",
    "                surname += vocab.lookup_index(sample_item)\n",
    "        \n",
    "        decoded_surnames.append(surname)\n",
    "    \n",
    "    return decoded_surnames # 2개의 해독된 성씨 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50276ed",
   "metadata": {},
   "source": [
    "## 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f8ebf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"훈련 상태를 업데이트합니다.\n",
    "\n",
    "    콤포넌트:\n",
    "     - 조기 종료: 과대 적합 방지\n",
    "     - 모델 체크포인트: 더 나은 모델을 저장합니다\n",
    "\n",
    "    :param args: 메인 매개변수\n",
    "    :param model: 훈련할 모델\n",
    "    :param train_state: 훈련 상태를 담은 딕셔너리\n",
    "    :returns:\n",
    "        새로운 훈련 상태\n",
    "    \"\"\"\n",
    "\n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= loss_tm1:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    텐서 크기 정규화\n",
    "    - 시퀀스의 타임 스텝마다 예측 만들기 때문에\n",
    "    - 3차원 텐서를 2차원 텐서로 변환해야 한다.\n",
    "    \n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 출력\n",
    "            3차원 텐서이면 행렬로 변환합니다.\n",
    "        y_true (torch.Tensor): 타깃 예측\n",
    "            행렬이면 벡터로 변환합니다.\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    # 예측 성씨 문자열 시퀀스는 3차원에서 2차원으로 \n",
    "    # 타깃 데이터는 2차원에서 1차원으로 정규화\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    \n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    # 마스킹 인덱스 0이 아닌 값들\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    # 1 * 1 인 값들만 합산--> 예측 맞은 개수\n",
    "    \n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100  # 정확도(%)\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    \"\"\"\n",
    "    가변 길이 시퀀스--> 마스킹된 위치에서는 손실 계산 안함\n",
    "    \"\"\"\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3613c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01283fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 경로: \n",
      "\tmodel_storage/ch7/model1_unconditioned_surname_generation\\vectorizer.json\n",
      "\tmodel_storage/ch7/model1_unconditioned_surname_generation\\model.pth\n",
      "CUDA 사용여부: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    surname_csv=\"data/surnames/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/ch7/model1_unconditioned_surname_generation\",\n",
    "    \n",
    "    # 모델 하이퍼파라미터\n",
    "    char_embedding_size=32,\n",
    "    rnn_hidden_size=32,\n",
    "    \n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    \n",
    "    # 실행 옵션\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"파일 경로: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "    \n",
    "# CUDA 체크\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"CUDA 사용여부: {}\".format(args.cuda))\n",
    "\n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 디렉토리 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "581f12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.reload_from_files:\n",
    "    # 체크포인트를 로드합니다.\n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
    "                                                              args.vectorizer_file)\n",
    "else:\n",
    "    # 데이터셋과 Vectorizer를 만듭니다.\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "model = SurnameGenerationModel(char_embedding_size=args.char_embedding_size, # 32\n",
    "                               char_vocab_size=len(vectorizer.char_vocab),   # 88\n",
    "                               rnn_hidden_size=args.rnn_hidden_size,         # 32\n",
    "                               padding_idx=vectorizer.char_vocab.mask_index) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939c559",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee6d4510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70909ca5ad94db7a870a539edf88bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587f0403c3984c839b64058488e59ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e110b3e38dfa474984720b0c26cd1c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "model = model.to(args.device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm.notebook.tqdm(desc='training routine', \n",
    "                               total=args.num_epochs,\n",
    "                               position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm.notebook.tqdm(desc='split=train',\n",
    "                               total=dataset.get_num_batches(args.batch_size), \n",
    "                               position=1, \n",
    "                               leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm.notebook.tqdm(desc='split=val',\n",
    "                             total=dataset.get_num_batches(args.batch_size), \n",
    "                             position=1, \n",
    "                             leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs): # 0~99\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # 훈련 세트에 대한 순회\n",
    "\n",
    "        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 훈련 과정은 5단계로 이루어집니다\n",
    "\n",
    "            # --------------------------------------\n",
    "            # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 단계 2. 출력을 계산합니다\n",
    "            y_pred = model(x_in=batch_dict['x_data']) # 모델에 2차원 텐서 입력 \n",
    "\n",
    "            # 단계 3. 손실을 계산합니다\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "            \"\"\"\n",
    "            예측값은 2차원으로, 타깃값은 1차원으로 정규화시키고,\n",
    "            \"가변길이시퀀싱\"으로 마스킹된 곳은 손실 계산 건너뜀\n",
    "            \"\"\"\n",
    "\n",
    "            # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "            loss.backward()\n",
    "\n",
    "            # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            \n",
    "            # 이동 손실과 이동 정확도를 계산합니다\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # 진행 상태 막대 업데이트\n",
    "            train_bar.set_postfix(loss=running_loss,\n",
    "                                  acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 검증 세트에 대한 순회\n",
    "\n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 단계 1. 출력을 계산합니다\n",
    "            y_pred = model(x_in=batch_dict['x_data'])\n",
    "\n",
    "            # 단계 2. 손실을 계산합니다\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "            # 단계 3. 이동 손실과 이동 정확도를 계산합니다\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # 진행 상태 막대 업데이트\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=model, \n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "        \n",
    "        # 샘플링을 위해 모델을 cpu로 옮깁니다\n",
    "        model = model.cpu()\n",
    "        sampled_surnames = decode_samples(\n",
    "            sample_from_model(model, vectorizer, num_samples=2), \n",
    "            vectorizer)\n",
    "        epoch_bar.set_postfix(sample1=sampled_surnames[0], \n",
    "                              sample2=sampled_surnames[1])\n",
    "        # 원래 장치로 모델을 이동합니다\n",
    "        model = model.to(args.device)\n",
    "        \n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"반복 중지\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7335d",
   "metadata": {},
   "source": [
    "## 모델 평가 \n",
    "### 테스트셋 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc9bcb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "model = model.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_acc = 0.\n",
    "model.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred = model(x_in=batch_dict['x_data'])\n",
    "\n",
    "    # 손실을 계산합니다\n",
    "    loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "    # 이동 손실과 이동 정확도를 계산합니다\n",
    "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss \n",
    "train_state['test_acc'] = running_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e163d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 2.5521657864252725;\n",
      "테스트 정확도: 24.958350915025292\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6838e",
   "metadata": {},
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de1dbbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ja\n",
      "OGri\n",
      "Coumas\n",
      "Wa\n",
      "Ed\n",
      "Nocotky\n",
      "Kishfskoocsl\n",
      "Sromn\n",
      "Shiletli\n",
      "Matlon\n"
     ]
    }
   ],
   "source": [
    "# 생성할 이름 개수\n",
    "num_names = 10\n",
    "model = model.cpu()\n",
    "# 이름 생성\n",
    "sampled_surnames = decode_samples(\n",
    "    sample_from_model(model, vectorizer, num_samples=num_names), \n",
    "    vectorizer)\n",
    "# 결과 출력\n",
    "print (\"-\"*15)\n",
    "for i in range(num_names):\n",
    "    print (sampled_surnames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ff2f2",
   "metadata": {},
   "source": [
    "조건 없는 (국적 인덱스 이용 안하는) 모델의 결과,\n",
    "의미 없는 시퀀스의 성씨들이 만들어짐\n",
    "\n",
    "--> 조건 있는 버전의 모델 사용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
